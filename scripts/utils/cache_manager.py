#!/usr/bin/env python3
"""
ç¼“å­˜ç®¡ç†å™¨

åŠŸèƒ½ï¼š
- ç®¡ç†åˆ†æç»“æœçš„ç¼“å­˜
- æ”¯æŒå¢é‡æ›´æ–°
- ç¼“å­˜æ–‡ä»¶ç®¡ç†
- ç¼“å­˜æœ‰æ•ˆæ€§éªŒè¯
"""

import json
import os
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "scripts/output/cache"):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ç¼“å­˜æ–‡ä»¶è·¯å¾„
        self.analysis_cache_file = self.cache_dir / "analysis_cache.json"
        self.article_hashes_file = self.cache_dir / "article_hashes.json"
        self.cache_metadata_file = self.cache_dir / "cache_metadata.json"
    
    def get_article_hash(self, content: str) -> str:
        """
        è®¡ç®—æ–‡ç« å†…å®¹çš„å“ˆå¸Œå€¼
        
        Args:
            content: æ–‡ç« å†…å®¹
            
        Returns:
            å†…å®¹å“ˆå¸Œå€¼
        """
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def load_article_hashes(self) -> Dict[str, str]:
        """
        åŠ è½½æ–‡ç« å“ˆå¸Œå€¼ç¼“å­˜
        
        Returns:
            æ–‡ç« è·¯å¾„åˆ°å“ˆå¸Œå€¼çš„æ˜ å°„
        """
        if self.article_hashes_file.exists():
            try:
                with open(self.article_hashes_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸  åŠ è½½æ–‡ç« å“ˆå¸Œç¼“å­˜å¤±è´¥: {e}")
                return {}
        return {}
    
    def save_article_hashes(self, article_hashes: Dict[str, str]):
        """
        ä¿å­˜æ–‡ç« å“ˆå¸Œå€¼ç¼“å­˜
        
        Args:
            article_hashes: æ–‡ç« è·¯å¾„åˆ°å“ˆå¸Œå€¼çš„æ˜ å°„
        """
        try:
            with open(self.article_hashes_file, 'w', encoding='utf-8') as f:
                json.dump(article_hashes, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜æ–‡ç« å“ˆå¸Œç¼“å­˜å¤±è´¥: {e}")
    
    def get_changed_articles(self, current_hashes: Dict[str, str]) -> List[str]:
        """
        è·å–å‘ç”Ÿå˜åŒ–çš„æ–‡ç« åˆ—è¡¨
        
        Args:
            current_hashes: å½“å‰æ–‡ç« å“ˆå¸Œå€¼
            
        Returns:
            å‘ç”Ÿå˜åŒ–çš„æ–‡ç« è·¯å¾„åˆ—è¡¨
        """
        cached_hashes = self.load_article_hashes()
        changed_articles = []
        
        for article_path, current_hash in current_hashes.items():
            cached_hash = cached_hashes.get(article_path)
            if cached_hash != current_hash:
                changed_articles.append(article_path)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ é™¤çš„æ–‡ç« 
        for cached_path in cached_hashes.keys():
            if cached_path not in current_hashes:
                print(f"ğŸ“ æ£€æµ‹åˆ°åˆ é™¤çš„æ–‡ç« : {cached_path}")
        
        return changed_articles
    
    def load_analysis_cache(self) -> Optional[Dict[str, Any]]:
        """
        åŠ è½½åˆ†æç»“æœç¼“å­˜
        
        Returns:
            ç¼“å­˜çš„åˆ†æç»“æœï¼Œå¦‚æœä¸å­˜åœ¨æˆ–è¿‡æœŸåˆ™è¿”å›None
        """
        if not self.analysis_cache_file.exists():
            return None
        
        try:
            with open(self.analysis_cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
            if self._is_cache_valid(cache_data):
                print("ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„åˆ†æç»“æœ")
                return cache_data
            else:
                print("â° ç¼“å­˜å·²è¿‡æœŸï¼Œéœ€è¦é‡æ–°åˆ†æ")
                return None
                
        except Exception as e:
            print(f"âš ï¸  åŠ è½½åˆ†æç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def save_analysis_cache(self, analysis_results: Dict[str, Any]):
        """
        ä¿å­˜åˆ†æç»“æœåˆ°ç¼“å­˜
        
        Args:
            analysis_results: åˆ†æç»“æœ
        """
        try:
            # æ·»åŠ ç¼“å­˜å…ƒæ•°æ®
            cache_data = {
                'analysis_results': analysis_results,
                'cached_at': datetime.now().isoformat(),
                'cache_version': '1.0'
            }
            
            with open(self.analysis_cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            print("ğŸ’¾ åˆ†æç»“æœå·²ç¼“å­˜")
            
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜åˆ†æç¼“å­˜å¤±è´¥: {e}")
    
    def _is_cache_valid(self, cache_data: Dict[str, Any]) -> bool:
        """
        æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            cache_data: ç¼“å­˜æ•°æ®
            
        Returns:
            ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            cached_at = cache_data.get('cached_at')
            if not cached_at:
                return False
            
            # è§£æç¼“å­˜æ—¶é—´
            cache_time = datetime.fromisoformat(cached_at)
            current_time = datetime.now()
            
            # ç¼“å­˜æœ‰æ•ˆæœŸï¼š7å¤©
            cache_validity = timedelta(days=7)
            
            return (current_time - cache_time) < cache_validity
            
        except Exception:
            return False
    
    def get_cache_info(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ä¿¡æ¯
        
        Returns:
            ç¼“å­˜ä¿¡æ¯å­—å…¸
        """
        cache_info = {
            'cache_dir': str(self.cache_dir),
            'analysis_cache_exists': self.analysis_cache_file.exists(),
            'article_hashes_exists': self.article_hashes_file.exists(),
            'cache_metadata_exists': self.cache_metadata_file.exists()
        }
        
        if self.analysis_cache_file.exists():
            try:
                with open(self.analysis_cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                cached_at = cache_data.get('cached_at', '')
                if cached_at:
                    cache_time = datetime.fromisoformat(cached_at)
                    current_time = datetime.now()
                    age = current_time - cache_time
                    
                    cache_info.update({
                        'cached_at': cached_at,
                        'cache_age_hours': round(age.total_seconds() / 3600, 1),
                        'is_valid': self._is_cache_valid(cache_data)
                    })
                    
            except Exception as e:
                cache_info['error'] = str(e)
        
        return cache_info
    
    def clear_cache(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        try:
            if self.analysis_cache_file.exists():
                os.remove(self.analysis_cache_file)
                print("ğŸ—‘ï¸  å·²æ¸…é™¤åˆ†æç»“æœç¼“å­˜")
            
            if self.article_hashes_file.exists():
                os.remove(self.article_hashes_file)
                print("ğŸ—‘ï¸  å·²æ¸…é™¤æ–‡ç« å“ˆå¸Œç¼“å­˜")
            
            if self.cache_metadata_file.exists():
                os.remove(self.cache_metadata_file)
                print("ğŸ—‘ï¸  å·²æ¸…é™¤ç¼“å­˜å…ƒæ•°æ®")
                
        except Exception as e:
            print(f"âš ï¸  æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")
    
    def update_cache_metadata(self, metadata: Dict[str, Any]):
        """
        æ›´æ–°ç¼“å­˜å…ƒæ•°æ®
        
        Args:
            metadata: å…ƒæ•°æ®å­—å…¸
        """
        try:
            # æ·»åŠ æ›´æ–°æ—¶é—´
            metadata['updated_at'] = datetime.now().isoformat()
            
            with open(self.cache_metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"âš ï¸  æ›´æ–°ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def get_incremental_analysis_data(self, all_articles: List[Dict], 
                                     current_hashes: Dict[str, str]) -> Tuple[List[Dict], Optional[Dict]]:
        """
        è·å–å¢é‡åˆ†ææ‰€éœ€çš„æ•°æ®
        
        Args:
            all_articles: æ‰€æœ‰æ–‡ç« åˆ—è¡¨
            current_hashes: å½“å‰æ–‡ç« å“ˆå¸Œå€¼
            
        Returns:
            (éœ€è¦åˆ†æçš„æ–‡ç« åˆ—è¡¨, ç¼“å­˜çš„åˆ†æç»“æœ)
        """
        # åŠ è½½ç¼“å­˜
        cached_results = self.load_analysis_cache()
        
        if not cached_results:
            # æ²¡æœ‰ç¼“å­˜ï¼Œéœ€è¦åˆ†ææ‰€æœ‰æ–‡ç« 
            print("ğŸ”„ æ— ç¼“å­˜ï¼Œå°†åˆ†ææ‰€æœ‰æ–‡ç« ")
            return all_articles, None
        
        # æ£€æŸ¥å“ªäº›æ–‡ç« å‘ç”Ÿäº†å˜åŒ–
        changed_articles = self.get_changed_articles(current_hashes)
        
        if not changed_articles:
            print("âœ… æ‰€æœ‰æ–‡ç« å‡æ— å˜åŒ–ï¼Œä½¿ç”¨ç¼“å­˜ç»“æœ")
            return [], cached_results.get('analysis_results')
        
        # åªåˆ†æå‘ç”Ÿå˜åŒ–çš„æ–‡ç« 
        print(f"ğŸ”„ æ£€æµ‹åˆ° {len(changed_articles)} ç¯‡æ–‡ç« å‘ç”Ÿå˜åŒ–ï¼Œè¿›è¡Œå¢é‡åˆ†æ")
        
        # ç­›é€‰éœ€è¦åˆ†æçš„æ–‡ç« 
        articles_to_analyze = [
            article for article in all_articles 
            if article.get('file_path') in changed_articles
        ]
        
        return articles_to_analyze, cached_results.get('analysis_results')
    
    def merge_incremental_results(self, cached_results: Dict[str, Any], 
                                new_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆå¹¶å¢é‡åˆ†æç»“æœ
        
        Args:
            cached_results: ç¼“å­˜çš„åˆ†æç»“æœ
            new_analysis: æ–°çš„åˆ†æç»“æœ
            
        Returns:
            åˆå¹¶åçš„ç»“æœ
        """
        print("ğŸ”— æ­£åœ¨åˆå¹¶å¢é‡åˆ†æç»“æœ...")
        
        # æ·±æ‹·è´ç¼“å­˜ç»“æœ
        merged_results = json.loads(json.dumps(cached_results))
        
        # æ›´æ–°å…ƒæ•°æ®
        merged_results['metadata']['updated_at'] = datetime.now().isoformat()
        merged_results['metadata']['incremental_update'] = True
        
        # æ›´æ–°æ‘˜è¦ä¿¡æ¯ï¼ˆéœ€è¦é‡æ–°è®¡ç®—ï¼‰
        if 'summary' in new_analysis:
            merged_results['summary'] = new_analysis['summary']
        
        # æ›´æ–°AIæ´å¯Ÿï¼ˆä½¿ç”¨æœ€æ–°çš„AIåˆ†æç»“æœï¼‰
        if 'ai_insights' in new_analysis:
            merged_results['ai_insights'] = new_analysis['ai_insights']
        
        # æ›´æ–°ç»¼åˆè¯„åˆ†
        if 'comprehensive_scores' in new_analysis:
            merged_results['comprehensive_scores'] = new_analysis['comprehensive_scores']
        
        # æ›´æ–°æ—¶é—´è¶‹åŠ¿ï¼ˆéœ€è¦é‡æ–°è®¡ç®—ï¼‰
        if 'time_trends' in new_analysis:
            merged_results['time_trends'] = new_analysis['time_trends']
        
        return merged_results
    
    def export_cache_report(self, output_path: str = None) -> str:
        """
        å¯¼å‡ºç¼“å­˜æŠ¥å‘Š
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not output_path:
            output_path = f"scripts/output/cache_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            cache_info = self.get_cache_info()
            
            report = {
                'cache_info': cache_info,
                'generated_at': datetime.now().isoformat(),
                'cache_directory': str(self.cache_dir),
                'files': []
            }
            
            # åˆ—å‡ºç¼“å­˜ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
            for file_path in self.cache_dir.iterdir():
                if file_path.is_file():
                    file_info = {
                        'name': file_path.name,
                        'size_bytes': file_path.stat().st_size,
                        'modified_at': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
                    }
                    report['files'].append(file_info)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“Š ç¼“å­˜æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {output_path}")
            return output_path
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºç¼“å­˜æŠ¥å‘Šå¤±è´¥: {e}")
            return None

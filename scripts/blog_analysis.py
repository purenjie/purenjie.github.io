#!/usr/bin/env python3
"""
åšå®¢åˆ†æä¸»è„šæœ¬

åŠŸèƒ½ï¼š
- è¯»å–æ‰€æœ‰åšå®¢æ–‡ç« 
- è¿›è¡ŒåŸºç¡€ç»Ÿè®¡åˆ†æ
- è°ƒç”¨AIè¿›è¡Œæ·±åº¦åˆ†æ
- æ•´åˆåˆ†æç»“æœ
- ç”Ÿæˆåˆ†ææŠ¥å‘Š
- æ”¯æŒå¢é‡æ›´æ–°å’Œç¼“å­˜

ä½¿ç”¨ç¤ºä¾‹ï¼š
  python blog_analysis.py
  python blog_analysis.py --force  # å¼ºåˆ¶é‡æ–°åˆ†æ
  python blog_analysis.py --cache-info  # æŸ¥çœ‹ç¼“å­˜ä¿¡æ¯
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from analyzers import BasicAnalyzer, AIAnalyzer, ResultIntegrator
from utils import TextProcessor, CacheManager


class BlogAnalyzer:
    """åšå®¢åˆ†æå™¨ä¸»ç±»"""
    
    def __init__(self, blog_dir: str = "src/content/blog", force_analysis: bool = False):
        """
        åˆå§‹åŒ–åšå®¢åˆ†æå™¨
        
        Args:
            blog_dir: åšå®¢æ–‡ç« ç›®å½•
            force_analysis: æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ†æ
        """
        self.blog_dir = Path(blog_dir)
        self.force_analysis = force_analysis
        
        # æ£€æŸ¥åšå®¢ç›®å½•æ˜¯å¦å­˜åœ¨
        if not self.blog_dir.exists():
            raise FileNotFoundError(f"åšå®¢ç›®å½•ä¸å­˜åœ¨: {self.blog_dir}")
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.text_processor = TextProcessor()
        self.basic_analyzer = BasicAnalyzer()
        self.ai_analyzer = AIAnalyzer()
        self.result_integrator = ResultIntegrator()
        self.cache_manager = CacheManager()
        
        print(f"ğŸ“š åšå®¢åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ åšå®¢ç›®å½•: {self.blog_dir}")
        print(f"ğŸ”„ å¼ºåˆ¶åˆ†æ: {'æ˜¯' if force_analysis else 'å¦'}")
    
    def discover_articles(self) -> List[Dict[str, Any]]:
        """
        å‘ç°æ‰€æœ‰åšå®¢æ–‡ç« 
        
        Returns:
            æ–‡ç« ä¿¡æ¯åˆ—è¡¨
        """
        print("ğŸ” æ­£åœ¨å‘ç°åšå®¢æ–‡ç« ...")
        
        articles = []
        
        # éå†åšå®¢ç›®å½•
        for item in self.blog_dir.iterdir():
            if item.is_file() and item.suffix == '.md':
                # å•æ–‡ä»¶Markdownæ–‡ç« 
                article_info = self._process_single_article(item)
                if article_info:
                    articles.append(article_info)
                    
            elif item.is_dir():
                # ç›®å½•æ ¼å¼æ–‡ç« 
                index_file = item / "index.md"
                if index_file.exists():
                    article_info = self._process_directory_article(item, index_file)
                    if article_info:
                        articles.append(article_info)
        
        print(f"âœ… å‘ç° {len(articles)} ç¯‡åšå®¢æ–‡ç« ")
        return articles
    
    def _process_single_article(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """
        å¤„ç†å•æ–‡ä»¶æ–‡ç« 
        
        Args:
            file_path: æ–‡ç« æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ç« ä¿¡æ¯å­—å…¸
        """
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ–‡æœ¬é¢„å¤„ç†
            cleaned_content, metadata, paragraphs, sentences = self.text_processor.process_article(content)
            
            # åŸºç¡€ç»Ÿè®¡
            stats = self.text_processor.get_text_statistics(cleaned_content)
            
            # æ„å»ºæ–‡ç« ä¿¡æ¯
            article_info = {
                'file_path': str(file_path),
                'title': metadata.get('title', file_path.stem) if metadata else file_path.stem,
                'publish_date': metadata.get('publishDate', ''),
                'tags': metadata.get('tags', []),
                'content': cleaned_content,
                'paragraphs': paragraphs,
                'sentences': sentences,
                'word_count': stats['total_words'],
                'char_count': stats['total_characters'],
                'paragraph_count': stats['paragraph_count'],
                'sentence_count': stats['sentence_count']
            }
            
            return article_info
            
        except Exception as e:
            print(f"âš ï¸  å¤„ç†æ–‡ç« å¤±è´¥ {file_path}: {e}")
            return None
    
    def _process_directory_article(self, dir_path: Path, index_file: Path) -> Optional[Dict[str, Any]]:
        """
        å¤„ç†ç›®å½•æ ¼å¼æ–‡ç« 
        
        Args:
            dir_path: æ–‡ç« ç›®å½•è·¯å¾„
            index_file: ç´¢å¼•æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ç« ä¿¡æ¯å­—å…¸
        """
        try:
            # è¯»å–ç´¢å¼•æ–‡ä»¶å†…å®¹
            with open(index_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ–‡æœ¬é¢„å¤„ç†
            cleaned_content, metadata, paragraphs, sentences = self.text_processor.process_article(content)
            
            # åŸºç¡€ç»Ÿè®¡
            stats = self.text_processor.get_text_statistics(cleaned_content)
            
            # æ„å»ºæ–‡ç« ä¿¡æ¯
            article_info = {
                'file_path': str(index_file),
                'title': metadata.get('title', dir_path.name) if metadata else dir_path.name,
                'publish_date': metadata.get('publishDate', ''),
                'tags': metadata.get('tags', []),
                'content': cleaned_content,
                'paragraphs': paragraphs,
                'sentences': sentences,
                'word_count': stats['total_words'],
                'char_count': stats['total_characters'],
                'paragraph_count': stats['paragraph_count'],
                'sentence_count': stats['sentence_count']
            }
            
            return article_info
            
        except Exception as e:
            print(f"âš ï¸  å¤„ç†æ–‡ç« å¤±è´¥ {index_file}: {e}")
            return None
    
    def analyze_articles(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åˆ†ææ–‡ç« åˆ—è¡¨
        
        Args:
            articles: æ–‡ç« ä¿¡æ¯åˆ—è¡¨
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        print("ğŸ“Š å¼€å§‹åˆ†ææ–‡ç« ...")
        
        if not articles:
            print("âš ï¸  æ²¡æœ‰æ–‡ç« éœ€è¦åˆ†æ")
            return {}
        
        # è®¡ç®—æ–‡ç« å“ˆå¸Œå€¼
        current_hashes = {}
        for article in articles:
            content = article.get('content', '')
            current_hashes[article['file_path']] = self.cache_manager.get_article_hash(content)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¢é‡åˆ†æ
        if not self.force_analysis:
            articles_to_analyze, cached_results = self.cache_manager.get_incremental_analysis_data(
                articles, current_hashes
            )
            
            if not articles_to_analyze and cached_results:
                # ä½¿ç”¨ç¼“å­˜ç»“æœ
                return cached_results
        else:
            articles_to_analyze = articles
            cached_results = None
        
        # åŸºç¡€ç»Ÿè®¡åˆ†æ
        print("ğŸ“ˆ æ­£åœ¨è¿›è¡ŒåŸºç¡€ç»Ÿè®¡åˆ†æ...")
        basic_analysis = self._perform_basic_analysis(articles_to_analyze)
        
        # AIæ·±åº¦åˆ†æ
        print("ğŸ¤– æ­£åœ¨è¿›è¡ŒAIæ·±åº¦åˆ†æ...")
        ai_analysis = self._perform_ai_analysis(articles_to_analyze, basic_analysis)
        
        # æ•´åˆåˆ†æç»“æœ
        print("ğŸ”— æ­£åœ¨æ•´åˆåˆ†æç»“æœ...")
        if cached_results and not self.force_analysis:
            # å¢é‡æ›´æ–°ï¼šåˆå¹¶ç¼“å­˜ç»“æœå’Œæ–°åˆ†æç»“æœ
            final_results = self.cache_manager.merge_incremental_results(
                cached_results, 
                {'summary': basic_analysis.get('summary', {}), 'ai_insights': ai_analysis}
            )
        else:
            # å®Œæ•´åˆ†æï¼šæ•´åˆæ‰€æœ‰ç»“æœ
            final_results = self.result_integrator.integrate_analysis_results(
                basic_analysis, ai_analysis, articles
            )
        
        # ä¿å­˜åˆ°ç¼“å­˜
        self.cache_manager.save_analysis_cache(final_results)
        
        # ä¿å­˜æ–‡ç« å“ˆå¸Œå€¼
        self.cache_manager.save_article_hashes(current_hashes)
        
        return final_results
    
    def _perform_basic_analysis(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ‰§è¡ŒåŸºç¡€ç»Ÿè®¡åˆ†æ
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            
        Returns:
            åŸºç¡€åˆ†æç»“æœ
        """
        # åˆå¹¶æ‰€æœ‰æ–‡ç« å†…å®¹è¿›è¡Œåˆ†æ
        combined_content = ""
        combined_paragraphs = []
        combined_sentences = []
        
        for article in articles:
            combined_content += article.get('content', '') + "\n\n"
            combined_paragraphs.extend(article.get('paragraphs', []))
            combined_sentences.extend(article.get('sentences', []))
        
        # æ‰§è¡Œç»¼åˆåˆ†æ
        basic_analysis = self.basic_analyzer.comprehensive_analysis(
            combined_content, combined_sentences, combined_paragraphs
        )
        
        # æ·»åŠ æ‘˜è¦ä¿¡æ¯
        total_words = sum(article.get('word_count', 0) for article in articles)
        total_chars = sum(article.get('char_count', 0) for article in articles)
        
        basic_analysis['summary'] = {
            'total_articles': len(articles),
            'total_words': total_words,
            'total_characters': total_chars,
            'avg_words_per_article': round(total_words / len(articles), 1) if articles else 0
        }
        
        return basic_analysis
    
    def _perform_ai_analysis(self, articles: List[Dict[str, Any]], 
                           basic_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡ŒAIæ·±åº¦åˆ†æ
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            basic_analysis: åŸºç¡€åˆ†æç»“æœ
            
        Returns:
            AIåˆ†æç»“æœ
        """
        # é€‰æ‹©ä»£è¡¨æ€§æ–‡ç« æ ·æœ¬ï¼ˆå‰3ç¯‡ï¼‰
        sample_articles = articles[:3]
        text_samples = [article.get('content', '') for article in sample_articles]
        
        # å‡†å¤‡åŸºç¡€ç»Ÿè®¡æ•°æ®
        basic_stats = {
            'total_words': basic_analysis.get('summary', {}).get('total_words', 0),
            'avg_sentence_length': basic_analysis.get('sentences', {}).get('avg_sentence_length', 0),
            'type_token_ratio': basic_analysis.get('vocabulary', {}).get('type_token_ratio', 0),
            'overall_sentiment': basic_analysis.get('emotions', {}).get('overall_sentiment', 'unknown'),
            'emotion_counts': basic_analysis.get('emotions', {}).get('emotion_counts', {}),
            'main_topic': basic_analysis.get('topics', {}).get('main_topic', 'unknown'),
            'topic_counts': basic_analysis.get('topics', {}).get('topic_counts', {}),
            'keywords': basic_analysis.get('topics', {}).get('keywords', []),
            'opening_patterns': basic_analysis.get('writing_patterns', {}).get('opening_patterns', []),
            'closing_patterns': basic_analysis.get('writing_patterns', {}).get('closing_patterns', []),
            'complexity_ratio': basic_analysis.get('sentences', {}).get('complexity', {}).get('complexity_ratio', 0),
            'paragraph_analysis': basic_analysis.get('writing_patterns', {}).get('paragraph_analysis', {}),
            'quotes': basic_analysis.get('writing_patterns', {}).get('writing_techniques', {}).get('quotes', 0),
            'emphasis': basic_analysis.get('writing_patterns', {}).get('writing_techniques', {}).get('emphasis', 0)
        }
        
        # æ‰§è¡ŒAIåˆ†æ
        ai_analysis = self.ai_analyzer.comprehensive_ai_analysis(text_samples, basic_stats)
        
        return ai_analysis
    
    def export_results(self, results: Dict[str, Any], output_path: str = None) -> str:
        """
        å¯¼å‡ºåˆ†æç»“æœ
        
        Args:
            results: åˆ†æç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not output_path:
            output_path = f"scripts/output/blog_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = Path(output_path).parent
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # å¯¼å‡ºç»“æœ
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… åˆ†æç»“æœå·²å¯¼å‡ºåˆ°: {output_path}")
            return output_path
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºç»“æœå¤±è´¥: {e}")
            return None
    
    def generate_summary_report(self, results: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        
        Args:
            results: åˆ†æç»“æœ
            
        Returns:
            æ‘˜è¦æŠ¥å‘Šæ–‡æœ¬
        """
        summary = []
        summary.append("=" * 60)
        summary.append("ğŸ“Š åšå®¢åˆ†ææ‘˜è¦æŠ¥å‘Š")
        summary.append("=" * 60)
        
        # åŸºæœ¬ä¿¡æ¯
        summary_info = results.get('summary', {})
        summary.append(f"ğŸ“š æ–‡ç« æ€»æ•°: {summary_info.get('total_articles', 0)}")
        summary.append(f"ğŸ“ æ€»å­—æ•°: {summary_info.get('total_words', 0):,}")
        summary.append(f"ğŸ“… æ—¶é—´è·¨åº¦: {summary_info.get('time_span', 'æœªçŸ¥')}")
        summary.append("")
        
        # å†™ä½œé£æ ¼è¯„åˆ†
        style_info = results.get('writing_style', {})
        summary.append("ğŸ¨ å†™ä½œé£æ ¼è¯„åˆ†:")
        summary.append(f"   è¯æ±‡ä¸°å¯Œåº¦: {style_info.get('vocabulary_richness', 0)}/10")
        summary.append(f"   å¥å­å¤æ‚åº¦: {style_info.get('sentence_complexity', 0)}/10")
        summary.append(f"   æƒ…æ„Ÿè¡¨è¾¾: {style_info.get('emotional_expression', 0)}/10")
        summary.append(f"   ç»“æ„ç»„ç»‡: {style_info.get('structure_organization', 0)}/10")
        summary.append(f"   æ€»ä½“è¯„åˆ†: {style_info.get('overall_style_score', 0)}/10")
        summary.append("")
        
        # æ€§æ ¼ç‰¹å¾
        personality_info = results.get('personality_traits', {})
        big_five_scores = personality_info.get('big_five_scores', {})
        if big_five_scores:
            summary.append("ğŸ§  æ€§æ ¼ç‰¹å¾ (Big Five):")
            trait_labels = {
                'openness': 'å¼€æ”¾æ€§',
                'conscientiousness': 'å°½è´£æ€§',
                'extraversion': 'å¤–å‘æ€§',
                'agreeableness': 'å®œäººæ€§',
                'neuroticism': 'ç¥ç»è´¨'
            }
            for trait, score in big_five_scores.items():
                label = trait_labels.get(trait, trait)
                summary.append(f"   {label}: {score}/100")
            summary.append("")
        
        # ç»¼åˆè¯„åˆ†
        comprehensive_scores = results.get('comprehensive_scores', {})
        if comprehensive_scores:
            summary.append("ğŸ† ç»¼åˆè¯„åˆ†:")
            summary.append(f"   å†™ä½œèƒ½åŠ›: {comprehensive_scores.get('writing_ability', 0)}/10")
            summary.append(f"   å†…å®¹è´¨é‡: {comprehensive_scores.get('content_quality', 0)}/10")
            summary.append(f"   ä¸ªæ€§å¹³è¡¡: {comprehensive_scores.get('personality_balance', 0)}/10")
            summary.append(f"   æ€»ä½“è¯„åˆ†: {comprehensive_scores.get('overall_score', 0)}/10")
            summary.append("")
            summary.append(f"ğŸ’¡ è¯„åˆ†è§£è¯»: {comprehensive_scores.get('score_interpretation', '')}")
            summary.append("")
        
        # ä¸»é¢˜åˆ†å¸ƒ
        content_analysis = results.get('content_analysis', {})
        topic_distribution = content_analysis.get('topic_distribution', {})
        if topic_distribution:
            summary.append("ğŸ“– ä¸»é¢˜åˆ†å¸ƒ:")
            for topic, ratio in topic_distribution.items():
                summary.append(f"   {topic}: {ratio:.1%}")
            summary.append("")
        
        # æ—¶é—´è¶‹åŠ¿
        time_trends = results.get('time_trends', {})
        if time_trends:
            summary.append("ğŸ“ˆ æ—¶é—´è¶‹åŠ¿:")
            word_trend = time_trends.get('word_count_trend', 'unknown')
            summary.append(f"   å­—æ•°å˜åŒ–è¶‹åŠ¿: {word_trend}")
            summary.append(f"   åˆ†æå‘¨æœŸ: {time_trends.get('analysis_period', 'æœªçŸ¥')}")
            summary.append("")
        
        summary.append("=" * 60)
        summary.append("ğŸ“‹ è¯¦ç»†åˆ†æç»“æœè¯·æŸ¥çœ‹å¯¼å‡ºçš„JSONæ–‡ä»¶")
        summary.append("ğŸŒ å»ºè®®å°†ç»“æœé›†æˆåˆ°Abouté¡µé¢å±•ç¤º")
        summary.append("=" * 60)
        
        return "\n".join(summary)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='åšå®¢åˆ†æå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python blog_analysis.py                    # æ­£å¸¸åˆ†æï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰
  python blog_analysis.py --force            # å¼ºåˆ¶é‡æ–°åˆ†æ
  python blog_analysis.py --cache-info       # æŸ¥çœ‹ç¼“å­˜ä¿¡æ¯
  python blog_analysis.py --clear-cache      # æ¸…é™¤ç¼“å­˜
  python blog_analysis.py --output path      # æŒ‡å®šè¾“å‡ºè·¯å¾„

ç¯å¢ƒå˜é‡:
  DOUBAO_API_KEY - ç«å±±å¼•æ“APIå¯†é’¥
        """
    )
    
    parser.add_argument('--force', action='store_true', 
                       help='å¼ºåˆ¶é‡æ–°åˆ†æï¼Œå¿½ç•¥ç¼“å­˜')
    parser.add_argument('--cache-info', action='store_true',
                       help='æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯')
    parser.add_argument('--clear-cache', action='store_true',
                       help='æ¸…é™¤æ‰€æœ‰ç¼“å­˜')
    parser.add_argument('--output', type=str,
                       help='æŒ‡å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--blog-dir', type=str, default='src/content/blog',
                       help='åšå®¢æ–‡ç« ç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    try:
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        if not os.getenv("DOUBAO_API_KEY"):
            print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°ç¯å¢ƒå˜é‡ DOUBAO_API_KEY")
            print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼šexport DOUBAO_API_KEY=your_api_key")
            sys.exit(1)
        
        # åˆ›å»ºåšå®¢åˆ†æå™¨
        analyzer = BlogAnalyzer(args.blog_dir, args.force)
        
        if args.cache_info:
            # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
            cache_info = analyzer.cache_manager.get_cache_info()
            print("ğŸ“‹ ç¼“å­˜ä¿¡æ¯:")
            print(json.dumps(cache_info, ensure_ascii=False, indent=2))
            return
        
        if args.clear_cache:
            # æ¸…é™¤ç¼“å­˜
            analyzer.cache_manager.clear_cache()
            print("âœ… ç¼“å­˜å·²æ¸…é™¤")
            return
        
        # æ‰§è¡Œåˆ†æ
        print("ğŸš€ å¼€å§‹åšå®¢åˆ†æ...")
        print("=" * 50)
        
        # å‘ç°æ–‡ç« 
        articles = analyzer.discover_articles()
        if not articles:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•åšå®¢æ–‡ç« ")
            sys.exit(1)
        
        # åˆ†ææ–‡ç« 
        results = analyzer.analyze_articles(articles)
        if not results:
            print("âŒ åˆ†æå¤±è´¥")
            sys.exit(1)
        
        # å¯¼å‡ºç»“æœ
        output_path = analyzer.export_results(results, args.output)
        
        # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        summary = analyzer.generate_summary_report(results)
        print("\n" + summary)
        
        print("\nğŸ‰ åšå®¢åˆ†æå®Œæˆï¼")
        if output_path:
            print(f"ğŸ“ ç»“æœæ–‡ä»¶: {output_path}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

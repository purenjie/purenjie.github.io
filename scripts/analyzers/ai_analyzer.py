#!/usr/bin/env python3
"""
AIæ·±åº¦åˆ†æå™¨

åŠŸèƒ½ï¼š
- è°ƒç”¨ç«å±±å¼•æ“è±†åŒ…APIè¿›è¡Œæ·±åº¦æ–‡æœ¬åˆ†æ
- åˆ†æå†™ä½œé£æ ¼å’Œä¸ªæ€§ç‰¹å¾
- ç”ŸæˆBig Fiveäººæ ¼ç‰¹è´¨åˆ†æ
- æä¾›æ€ç»´æ¨¡å¼å’Œå†™ä½œä¹ æƒ¯è§£è¯»
"""

import json
import os
import requests
from typing import Dict, List, Any, Optional
from datetime import datetime


class AIAnalyzer:
    """AIæ·±åº¦åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–AIåˆ†æå™¨"""
        # APIé…ç½®ï¼ˆå¤ç”¨ç°æœ‰é…ç½®ï¼‰
        self.ark_base_url = "https://ark.cn-beijing.volces.com/api/v3"
        self.text_model = "doubao-1-5-pro-32k-250115"
        
        # è·å–APIå¯†é’¥
        self.api_key = os.getenv("DOUBAO_API_KEY")
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ°ç¯å¢ƒå˜é‡ DOUBAO_API_KEY")
        
        # åˆ†æç»´åº¦é…ç½®
        self.analysis_dimensions = [
            'writing_style',
            'personality_traits', 
            'thinking_patterns',
            'content_preferences',
            'emotional_expression'
        ]
    
    def analyze_writing_style(self, text_samples: List[str], basic_stats: Dict) -> Dict[str, Any]:
        """
        åˆ†æå†™ä½œé£æ ¼
        
        Args:
            text_samples: æ–‡æœ¬æ ·æœ¬åˆ—è¡¨
            basic_stats: åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            å†™ä½œé£æ ¼åˆ†æç»“æœ
        """
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†™ä½œé£æ ¼åˆ†æå¸ˆï¼Œæ“…é•¿åˆ†æä¸­æ–‡åšå®¢çš„å†™ä½œç‰¹ç‚¹ã€‚

è¯·åŸºäºæä¾›çš„æ–‡æœ¬æ ·æœ¬å’Œç»Ÿè®¡æ•°æ®ï¼Œåˆ†æä½œè€…çš„å†™ä½œé£æ ¼ç‰¹å¾ã€‚åˆ†æè¦å®¢è§‚ã€å‡†ç¡®ï¼Œé¿å…è¿‡åº¦è§£è¯»ã€‚

åˆ†æç»´åº¦åŒ…æ‹¬ï¼š
1. è¯­è¨€ç‰¹è‰²ï¼šç”¨è¯åå¥½ã€å¥å¼ç‰¹ç‚¹ã€ä¿®è¾æ‰‹æ³•
2. ç»“æ„ç‰¹å¾ï¼šæ®µè½ç»„ç»‡ã€é€»è¾‘å±‚æ¬¡ã€è¿‡æ¸¡æ–¹å¼
3. è¡¨è¾¾é£æ ¼ï¼šç›´æ¥æ€§ã€æƒ…æ„Ÿæ€§ã€ä¸“ä¸šæ€§
4. è¯»è€…äº’åŠ¨ï¼šæ˜¯å¦è€ƒè™‘è¯»è€…æ„Ÿå—ã€æ˜¯å¦å¼•å¯¼æ€è€ƒ

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œæ ¼å¼è¦æ¸…æ™°æ˜“è¯»ã€‚"""
        
        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹åšå®¢æ–‡ç« çš„å†™ä½œé£æ ¼ï¼š

æ–‡æœ¬æ ·æœ¬ï¼ˆå‰3ç¯‡ï¼‰ï¼š
{chr(10).join([f"æ ·æœ¬{i+1}: {sample[:200]}..." for i, sample in enumerate(text_samples[:3])])}

åŸºç¡€ç»Ÿè®¡æ•°æ®ï¼š
- æ€»å­—æ•°ï¼š{basic_stats.get('total_words', 0)}
- å¹³å‡å¥é•¿ï¼š{basic_stats.get('avg_sentence_length', 0)}å­—ç¬¦
- è¯æ±‡å¤šæ ·æ€§ï¼š{basic_stats.get('type_token_ratio', 0)}
- æƒ…æ„Ÿå€¾å‘ï¼š{basic_stats.get('overall_sentiment', 'unknown')}

è¯·æä¾›è¯¦ç»†çš„å†™ä½œé£æ ¼åˆ†æã€‚"""
        
        return self._call_ai_api(system_prompt, user_prompt, "writing_style")
    
    def analyze_personality_traits(self, text_samples: List[str], basic_stats: Dict) -> Dict[str, Any]:
        """
        åˆ†ææ€§æ ¼ç‰¹å¾ï¼ˆåŸºäºBig Fiveæ¨¡å‹ï¼‰
        
        Args:
            text_samples: æ–‡æœ¬æ ·æœ¬åˆ—è¡¨
            basic_stats: åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            æ€§æ ¼ç‰¹å¾åˆ†æç»“æœ
        """
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æœ¬å¿ƒç†å­¦åˆ†æå¸ˆï¼Œæ“…é•¿é€šè¿‡å†™ä½œé£æ ¼åˆ†æä½œè€…çš„æ€§æ ¼ç‰¹å¾ã€‚

è¯·åŸºäºBig Fiveäººæ ¼æ¨¡å‹ï¼ˆå¼€æ”¾æ€§ã€å°½è´£æ€§ã€å¤–å‘æ€§ã€å®œäººæ€§ã€ç¥ç»è´¨ï¼‰æ¥åˆ†æä½œè€…çš„ä¸ªæ€§ç‰¹å¾ã€‚

åˆ†æè¦æ±‚ï¼š
1. æ¯ä¸ªç»´åº¦ç»™å‡º0-100çš„è¯„åˆ†
2. æä¾›è¯„åˆ†çš„å…·ä½“ä¾æ®
3. ç”¨ä¸­æ–‡æè¿°æ€§æ ¼ç‰¹ç‚¹
4. é¿å…åˆ»æ¿å°è±¡ï¼ŒåŸºäºæ–‡æœ¬è¯æ®åˆ†æ

è¯„åˆ†æ ‡å‡†ï¼š
- å¼€æ”¾æ€§ï¼šå¯¹æ–°äº‹ç‰©çš„æ¥å—åº¦ã€åˆ›æ–°æ€ç»´ã€æƒ³è±¡åŠ›
- å°½è´£æ€§ï¼šç»„ç»‡æ€§ã€ç›®æ ‡å¯¼å‘ã€è‡ªæˆ‘æ§åˆ¶
- å¤–å‘æ€§ï¼šç¤¾äº¤æ€§ã€è¡¨è¾¾æ€§ã€æ´»åŠ›æ°´å¹³
- å®œäººæ€§ï¼šåˆä½œæ€§ã€ä¿¡ä»»åº¦ã€åŒç†å¿ƒ
- ç¥ç»è´¨ï¼šæƒ…ç»ªç¨³å®šæ€§ã€å‹åŠ›åº”å¯¹ã€ç„¦è™‘æ°´å¹³"""
        
        user_prompt = f"""è¯·åŸºäºä»¥ä¸‹åšå®¢æ–‡ç« åˆ†æä½œè€…çš„æ€§æ ¼ç‰¹å¾ï¼š

æ–‡æœ¬æ ·æœ¬ï¼ˆå‰3ç¯‡ï¼‰ï¼š
{chr(10).join([f"æ ·æœ¬{i+1}: {sample[:200]}..." for i, sample in enumerate(text_samples[:3])])}

åŸºç¡€ç»Ÿè®¡æ•°æ®ï¼š
- æƒ…æ„Ÿè¯æ±‡ä½¿ç”¨ï¼š{basic_stats.get('emotion_counts', {}).get('positive', 0)}ä¸ªç§¯æè¯ï¼Œ{basic_stats.get('emotion_counts', {}).get('negative', 0)}ä¸ªæ¶ˆæè¯
- å†™ä½œæ¨¡å¼ï¼š{basic_stats.get('opening_patterns', [])}å¼€å¤´ï¼Œ{basic_stats.get('closing_patterns', [])}ç»“å°¾
- ä¸»é¢˜åå¥½ï¼š{basic_stats.get('main_topic', 'unknown')}

è¯·æŒ‰ç…§Big Fiveæ¨¡å‹ç»™å‡ºè¯„åˆ†å’Œè¯¦ç»†åˆ†æã€‚"""
        
        return self._call_ai_api(system_prompt, user_prompt, "personality_traits")
    
    def analyze_thinking_patterns(self, text_samples: List[str], basic_stats: Dict) -> Dict[str, Any]:
        """
        åˆ†ææ€ç»´æ¨¡å¼
        
        Args:
            text_samples: æ–‡æœ¬æ ·æœ¬åˆ—è¡¨
            basic_stats: åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            æ€ç»´æ¨¡å¼åˆ†æç»“æœ
        """
        system_prompt = """ä½ æ˜¯ä¸€ä½è®¤çŸ¥å¿ƒç†å­¦ä¸“å®¶ï¼Œæ“…é•¿åˆ†æä½œè€…çš„æ€ç»´æ¨¡å¼å’Œè®¤çŸ¥ç‰¹å¾ã€‚

è¯·åˆ†æä½œè€…çš„æ€ç»´ç‰¹ç‚¹ï¼ŒåŒ…æ‹¬ï¼š
1. æ€ç»´ç±»å‹ï¼šåˆ†æå‹ã€ç›´è§‰å‹ã€ç»éªŒå‹ã€ç†è®ºå‹
2. é—®é¢˜è§£å†³æ–¹å¼ï¼šç³»ç»Ÿæ€§ã€åˆ›é€ æ€§ã€å®ç”¨æ€§ã€åæ€æ€§
3. å­¦ä¹ é£æ ¼ï¼šè§†è§‰å‹ã€å¬è§‰å‹ã€åŠ¨æ‰‹å‹ã€é˜…è¯»å‹
4. å†³ç­–æ¨¡å¼ï¼šç†æ€§åˆ†æã€æƒ…æ„Ÿé©±åŠ¨ã€ç»éªŒä¾èµ–ã€åˆ›æ–°å°è¯•

åˆ†æè¦åŸºäºæ–‡æœ¬è¯æ®ï¼Œé¿å…ä¸»è§‚è‡†æµ‹ã€‚"""
        
        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹åšå®¢æ–‡ç« ä½œè€…çš„æ€ç»´æ¨¡å¼ï¼š

æ–‡æœ¬æ ·æœ¬ï¼ˆå‰3ç¯‡ï¼‰ï¼š
{chr(10).join([f"æ ·æœ¬{i+1}: {sample[:200]}..." for i, sample in enumerate(text_samples[:3])])}

åŸºç¡€ç»Ÿè®¡æ•°æ®ï¼š
- å¥å­å¤æ‚åº¦ï¼š{basic_stats.get('complexity_ratio', 0)}
- æ®µè½ç»“æ„ï¼š{basic_stats.get('paragraph_analysis', {}).get('avg_paragraph_length', 0)}å­—ç¬¦/æ®µ
- å†™ä½œæŠ€å·§ï¼šå¼•ç”¨{basic_stats.get('quotes', 0)}æ¬¡ï¼Œå¼ºè°ƒ{basic_stats.get('emphasis', 0)}æ¬¡

è¯·æä¾›è¯¦ç»†çš„æ€ç»´æ¨¡å¼åˆ†æã€‚"""
        
        return self._call_ai_api(system_prompt, user_prompt, "thinking_patterns")
    
    def analyze_content_preferences(self, text_samples: List[str], basic_stats: Dict) -> Dict[str, Any]:
        """
        åˆ†æå†…å®¹åå¥½
        
        Args:
            text_samples: æ–‡æœ¬æ ·æœ¬åˆ—è¡¨
            basic_stats: åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            å†…å®¹åå¥½åˆ†æç»“æœ
        """
        system_prompt = """ä½ æ˜¯ä¸€ä½å†…å®¹åˆ†æä¸“å®¶ï¼Œæ“…é•¿åˆ†æä½œè€…çš„å†™ä½œå†…å®¹å’Œåå¥½ã€‚

è¯·åˆ†æä½œè€…çš„å†…å®¹åå¥½ï¼ŒåŒ…æ‹¬ï¼š
1. ä¸»é¢˜é€‰æ‹©ï¼šæŠ€æœ¯ã€ç”Ÿæ´»ã€è¯»ä¹¦ã€æ€»ç»“ç­‰
2. å†…å®¹æ·±åº¦ï¼šæµ…å±‚ä»‹ç»ã€æ·±åº¦åˆ†æã€å®ç”¨æŒ‡å¯¼ã€ç†è®ºæ¢è®¨
3. è¡¨è¾¾æ–¹å¼ï¼šæ•…äº‹åŒ–ã€æ•°æ®åŒ–ã€å¯¹æ¯”å¼ã€æ¸è¿›å¼
4. ä»·å€¼å–å‘ï¼šå®ç”¨æ€§ã€å¯å‘æ€§ã€å¨±ä¹æ€§ã€æ•™è‚²æ€§

åˆ†æè¦å®¢è§‚å‡†ç¡®ï¼ŒåŸºäºæ–‡æœ¬å†…å®¹ã€‚"""
        
        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹åšå®¢æ–‡ç« ä½œè€…çš„å†…å®¹åå¥½ï¼š

æ–‡æœ¬æ ·æœ¬ï¼ˆå‰3ç¯‡ï¼‰ï¼š
{chr(10).join([f"æ ·æœ¬{i+1}: {sample[:200]}..." for i, sample in enumerate(text_samples[:3])])}

åŸºç¡€ç»Ÿè®¡æ•°æ®ï¼š
- ä¸»è¦ä¸»é¢˜ï¼š{basic_stats.get('main_topic', 'unknown')}
- å…³é”®è¯ï¼š{', '.join([word for word, _ in basic_stats.get('keywords', [])[:5]])}
- æŠ€æœ¯è¯æ±‡ï¼š{basic_stats.get('topic_counts', {}).get('technology', 0)}ä¸ª
- ç”Ÿæ´»è¯æ±‡ï¼š{basic_stats.get('topic_counts', {}).get('life', 0)}ä¸ª

è¯·æä¾›è¯¦ç»†çš„å†…å®¹åå¥½åˆ†æã€‚"""
        
        return self._call_ai_api(system_prompt, user_prompt, "content_preferences")
    
    def analyze_emotional_expression(self, text_samples: List[str], basic_stats: Dict) -> Dict[str, Any]:
        """
        åˆ†ææƒ…æ„Ÿè¡¨è¾¾
        
        Args:
            text_samples: æ–‡æœ¬æ ·æœ¬åˆ—è¡¨
            basic_stats: åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            æƒ…æ„Ÿè¡¨è¾¾åˆ†æç»“æœ
        """
        system_prompt = """ä½ æ˜¯ä¸€ä½æƒ…æ„Ÿåˆ†æä¸“å®¶ï¼Œæ“…é•¿åˆ†æä½œè€…çš„æƒ…æ„Ÿè¡¨è¾¾æ–¹å¼å’Œç‰¹ç‚¹ã€‚

è¯·åˆ†æä½œè€…çš„æƒ…æ„Ÿè¡¨è¾¾ç‰¹å¾ï¼ŒåŒ…æ‹¬ï¼š
1. æƒ…æ„ŸåŸºè°ƒï¼šç§¯æã€æ¶ˆæã€ä¸­æ€§ã€æ··åˆ
2. æƒ…æ„Ÿæ·±åº¦ï¼šè¡¨é¢ã€æ·±å…¥ã€å¤æ‚ã€ç®€å•
3. è¡¨è¾¾æ–¹å¼ï¼šç›´æ¥è¡¨è¾¾ã€é—´æ¥æš—ç¤ºã€å¯¹æ¯”è¡¬æ‰˜ã€ç¯å¢ƒçƒ˜æ‰˜
4. æƒ…æ„Ÿå˜åŒ–ï¼šç¨³å®šã€æ³¢åŠ¨ã€æ¸è¿›ã€è·³è·ƒ

åˆ†æè¦åŸºäºæ–‡æœ¬ä¸­çš„æƒ…æ„Ÿè¯æ±‡å’Œè¡¨è¾¾æ–¹å¼ã€‚"""
        
        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹åšå®¢æ–‡ç« ä½œè€…çš„æƒ…æ„Ÿè¡¨è¾¾ï¼š

æ–‡æœ¬æ ·æœ¬ï¼ˆå‰3ç¯‡ï¼‰ï¼š
{chr(10).join([f"æ ·æœ¬{i+1}: {sample[:200]}..." for i, sample in enumerate(text_samples[:3])])}

åŸºç¡€ç»Ÿè®¡æ•°æ®ï¼š
- æƒ…æ„Ÿå€¾å‘ï¼š{basic_stats.get('overall_sentiment', 'unknown')}
- ç§¯æè¯æ±‡ï¼š{basic_stats.get('emotion_counts', {}).get('positive', 0)}ä¸ª
- æ¶ˆæè¯æ±‡ï¼š{basic_stats.get('emotion_counts', {}).get('negative', 0)}ä¸ª
- ä¸­æ€§è¯æ±‡ï¼š{basic_stats.get('emotion_counts', {}).get('neutral', 0)}ä¸ª

è¯·æä¾›è¯¦ç»†çš„æƒ…æ„Ÿè¡¨è¾¾åˆ†æã€‚"""
        
        return self._call_ai_api(system_prompt, user_prompt, "emotional_expression")
    
    def comprehensive_ai_analysis(self, text_samples: List[str], basic_stats: Dict) -> Dict[str, Any]:
        """
        ç»¼åˆAIåˆ†æ
        
        Args:
            text_samples: æ–‡æœ¬æ ·æœ¬åˆ—è¡¨
            basic_stats: åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            ç»¼åˆAIåˆ†æç»“æœ
        """
        results = {}
        
        try:
            # å†™ä½œé£æ ¼åˆ†æ
            print("ğŸ¤– æ­£åœ¨è¿›è¡Œå†™ä½œé£æ ¼åˆ†æ...")
            results['writing_style'] = self.analyze_writing_style(text_samples, basic_stats)
            
            # æ€§æ ¼ç‰¹å¾åˆ†æ
            print("ğŸ§  æ­£åœ¨è¿›è¡Œæ€§æ ¼ç‰¹å¾åˆ†æ...")
            results['personality_traits'] = self.analyze_personality_traits(text_samples, basic_stats)
            
            # æ€ç»´æ¨¡å¼åˆ†æ
            print("ğŸ’­ æ­£åœ¨è¿›è¡Œæ€ç»´æ¨¡å¼åˆ†æ...")
            results['thinking_patterns'] = self.analyze_thinking_patterns(text_samples, basic_stats)
            
            # å†…å®¹åå¥½åˆ†æ
            print("ğŸ“š æ­£åœ¨è¿›è¡Œå†…å®¹åå¥½åˆ†æ...")
            results['content_preferences'] = self.analyze_content_preferences(text_samples, basic_stats)
            
            # æƒ…æ„Ÿè¡¨è¾¾åˆ†æ
            print("ğŸ’– æ­£åœ¨è¿›è¡Œæƒ…æ„Ÿè¡¨è¾¾åˆ†æ...")
            results['emotional_expression'] = self.analyze_emotional_expression(text_samples, basic_stats)
            
            # æ·»åŠ åˆ†ææ—¶é—´æˆ³
            results['analysis_timestamp'] = datetime.now().isoformat()
            results['analysis_status'] = 'completed'
            
        except Exception as e:
            print(f"âŒ AIåˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            results['analysis_status'] = 'failed'
            results['error_message'] = str(e)
        
        return results
    
    def _call_ai_api(self, system_prompt: str, user_prompt: str, analysis_type: str) -> Dict[str, Any]:
        """
        è°ƒç”¨ç«å±±å¼•æ“è±†åŒ…API
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            analysis_type: åˆ†æç±»å‹
            
        Returns:
            APIå“åº”ç»“æœ
        """
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        data = {
            "model": self.text_model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": 1024,
            "temperature": 0.7
        }
        
        try:
            response = requests.post(
                f"{self.ark_base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=60
            )
            
            response.raise_for_status()
            result = response.json()
            
            if "choices" in result and len(result["choices"]) > 0:
                content = result["choices"][0]["message"]["content"].strip()
                
                # å°è¯•è§£æJSONæ ¼å¼çš„å“åº”
                try:
                    parsed_content = json.loads(content)
                    return {
                        "analysis_type": analysis_type,
                        "content": parsed_content,
                        "raw_content": content,
                        "status": "success"
                    }
                except json.JSONDecodeError:
                    # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
                    return {
                        "analysis_type": analysis_type,
                        "content": {"text": content},
                        "raw_content": content,
                        "status": "success"
                    }
            else:
                raise Exception("APIå“åº”æ ¼å¼å¼‚å¸¸")
                
        except requests.exceptions.RequestException as e:
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: {e}")
        except json.JSONDecodeError as e:
            raise Exception(f"è§£æAPIå“åº”å¤±è´¥: {e}")
        except Exception as e:
            raise Exception(f"æœªçŸ¥é”™è¯¯: {e}")
    
    def extract_personality_scores(self, personality_analysis: Dict) -> Dict[str, int]:
        """
        ä»æ€§æ ¼åˆ†æç»“æœä¸­æå–Big Fiveè¯„åˆ†
        
        Args:
            personality_analysis: æ€§æ ¼åˆ†æç»“æœ
            
        Returns:
            Big Fiveè¯„åˆ†å­—å…¸
        """
        scores = {
            'openness': 50,
            'conscientiousness': 50,
            'extraversion': 50,
            'agreeableness': 50,
            'neuroticism': 50
        }
        
        try:
            content = personality_analysis.get('content', {})
            
            # å°è¯•ä»ä¸åŒæ ¼å¼çš„å“åº”ä¸­æå–è¯„åˆ†
            if isinstance(content, dict):
                for trait in scores.keys():
                    # æŸ¥æ‰¾åŒ…å«è¯„åˆ†çš„å­—æ®µ
                    for key, value in content.items():
                        if trait in key.lower() and isinstance(value, (int, float)):
                            scores[trait] = min(100, max(0, int(value)))
                        elif isinstance(value, str) and trait in value.lower():
                            # å°è¯•ä»æ–‡æœ¬ä¸­æå–æ•°å­—
                            import re
                            numbers = re.findall(r'\d+', value)
                            if numbers:
                                scores[trait] = min(100, max(0, int(numbers[0])))
            
            # å¦‚æœæ— æ³•æå–ï¼Œä½¿ç”¨é»˜è®¤å€¼
            if all(score == 50 for score in scores.values()):
                print("âš ï¸  æ— æ³•ä»AIåˆ†æä¸­æå–å…·ä½“è¯„åˆ†ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                
        except Exception as e:
            print(f"âš ï¸  æå–æ€§æ ¼è¯„åˆ†æ—¶å‡ºé”™: {e}")
        
        return scores
